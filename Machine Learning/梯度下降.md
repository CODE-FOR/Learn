# 梯度下降

## 概念

对loss使用

重复进行$\theta_j:=\theta_j-\alpha\frac{\delta}{\delta \theta_j}J(\theta_0,\theta_1)\;(for\,j =0\,and\,j=1)$

> 当前仅考虑两个参数

- $\alpha$   learning rate
- 参数要 simultaneously update 同步更新

$J(\theta)$

![image-20220303153647569](C:\Users\liuzhenwei\AppData\Roaming\Typora\typora-user-images\image-20220303153647569.png)

>$\theta=\theta-\alpha \frac{d}{d\theta}J(\theta)$
>
>一般来说，越接近局部最优点，导数就越小，在最优点处导数为0，因此越接近局部最优点，即使学习率不变，$\theta$变化的幅度也越来越小，逐步趋近于局部最优点。

## 例子

### 线性回归的梯度下降

![image-20220303154356175](C:\Users\liuzhenwei\AppData\Roaming\Typora\typora-user-images\image-20220303154356175.png)

